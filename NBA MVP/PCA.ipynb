{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we make a first pass at an attempt to use PCA in order to make our variables orthogonal. I think this could be an important step as we have many highly correlated variables such as pts_per_game, pts_per_poss, and pts_per_mp. PCA will allow us to take full advantage of these variables while removing redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import logging\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import PolynomialFeatures, normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor, AdaBoostRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain= pd.read_csv('../Data Scraping/DataTrainSet.csv')\n",
    "dataVal = pd.read_csv('../Data Scraping/DataValidateSet.csv')\n",
    "dataTest = pd.read_csv('../Data Scraping/DataTestSet.csv')\n",
    "\n",
    "features = ['player','pos_x','age_x','team_id_x','g_x','mp_x','fg_per_poss','fga_per_poss','fg_pct_x'\n",
    "              ,'fg3_per_poss','fg3a_per_poss','fg3_pct_x','fg2_per_poss','fg2a_per_poss','fg2_pct_x','ft_per_poss'\n",
    "              ,'fta_per_poss','ft_pct_x','orb_per_poss','drb_per_poss','trb_per_poss','ast_per_poss','stl_per_poss'\n",
    "              ,'blk_per_poss','tov_per_poss','pf_per_poss','pts_per_poss','off_rtg','def_rtg','per','ts_pct'\n",
    "              ,'fg3a_per_fga_pct','fta_per_fga_pct','orb_pct','drb_pct','trb_pct','ast_pct','stl_pct','blk_pct'\n",
    "              ,'tov_pct','usg_pct','ows','dws','ws','ws_per_48','obpm','dbpm','bpm','vorp'\n",
    "              ,'mp_per_g','fg_per_g','fga_per_g','fg3_per_g','fg3a_per_g'\n",
    "              ,'fg2_per_g','fg2a_per_g','efg_pct','ft_per_g','fta_per_g','orb_per_g','drb_per_g','trb_per_g'\n",
    "              ,'ast_per_g','stl_per_g','blk_per_g','tov_per_g','pf_per_g','pts_per_g','fg_per_mp','fga_per_mp'\n",
    "              ,'fg3_per_mp','fg3a_per_mp','fg2_per_mp','fg2a_per_mp','ft_per_mp','fta_per_mp','orb_per_mp'\n",
    "              ,'drb_per_mp','trb_per_mp','ast_per_mp','stl_per_mp','blk_per_mp','tov_per_mp','pf_per_mp','pts_per_mp'\n",
    "              ,'votes_first','points_won']\n",
    "\n",
    "feat = features[4:-2]\n",
    "XTrainDF = dataTrain[feat]\n",
    "XValDF = dataVal[feat]\n",
    "XTestDF = dataTest[feat]\n",
    "\n",
    "XTrainDF = XTrainDF.fillna(0)\n",
    "XValDF = XValDF.fillna(0)\n",
    "XTestDF = XTestDF.fillna(0)\n",
    "\n",
    "XTrain = XTrainDF.to_numpy()\n",
    "XVal = XValDF.to_numpy()\n",
    "XTest = XTestDF.to_numpy()\n",
    "\n",
    "yTrain = dataTrain['points_won'].to_numpy()\n",
    "yVal = dataVal['points_won'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=20, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components = 20)\n",
    "pca.fit(XTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain_pca = pca.transform(XTrain)\n",
    "XVal_pca = pca.transform(XVal)\n",
    "XTest_pca = pca.transform(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ws', 'pts_per_g', 'vorp','fta_per_g', 'bpm', 'fg2_per_g','ows','ows','per','mp_per_g']\n",
    "\n",
    "\n",
    "# features = ['g_x','mp_x','fg_per_poss','fga_per_poss','fg_pct_x'\n",
    "#               ,'fg3_per_poss','fg3a_per_poss','fg3_pct_x','fg2_per_poss','fg2a_per_poss','fg2_pct_x','ft_per_poss'\n",
    "#               ,'fta_per_poss','ft_pct_x','orb_per_poss','drb_per_poss','trb_per_poss','ast_per_poss','stl_per_poss'\n",
    "#               ,'blk_per_poss','tov_per_poss','pf_per_poss','pts_per_poss','off_rtg','def_rtg','per','ts_pct'\n",
    "#               ,'fg3a_per_fga_pct','fta_per_fga_pct','orb_pct','drb_pct','trb_pct','ast_pct','stl_pct','blk_pct'\n",
    "#               ,'tov_pct','usg_pct','ows','dws','ws','ws_per_48','obpm','dbpm','bpm','vorp'\n",
    "#               ,'mp_per_g','fg_per_g','fga_per_g','fg3_per_g','fg3a_per_g'\n",
    "#               ,'fg2_per_g','fg2a_per_g','efg_pct','ft_per_g','fta_per_g','orb_per_g','drb_per_g','trb_per_g'\n",
    "#               ,'ast_per_g','stl_per_g','blk_per_g','tov_per_g','pf_per_g','pts_per_g','fg_per_mp','fga_per_mp'\n",
    "#               ,'fg3_per_mp','fg3a_per_mp','fg2_per_mp','fg2a_per_mp','ft_per_mp','fta_per_mp','orb_per_mp'\n",
    "#               ,'drb_per_mp','trb_per_mp','ast_per_mp','stl_per_mp','blk_per_mp','tov_per_mp','pf_per_mp','pts_per_mp']\n",
    "\n",
    "XTrainDF = dataTrain[features]\n",
    "XValDF = dataVal[features]\n",
    "XTestDF = dataTest[features]\n",
    "\n",
    "XTrainDF = XTrainDF.fillna(0)\n",
    "XValDF = XValDF.fillna(0)\n",
    "XTestDF = XTestDF.fillna(0)\n",
    "\n",
    "XTrain = XTrainDF.to_numpy()\n",
    "XVal = XValDF.to_numpy()\n",
    "XTest = XTestDF.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data_frame, estimators, params, filename, poly_fit):\n",
    "\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(filename=filename, filemode='w', level=logging.INFO)\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    minimal_error, best_estimator = None, None\n",
    "    \n",
    "    for estimator in estimators:\n",
    "        \n",
    "        try: \n",
    "            print(f\"Starting with estimator: {estimator.__name__}\")\n",
    "            logging.info(f\"Starting with estimator: {estimator.__name__}\")\n",
    "\n",
    "            for index, cur_params in enumerate(params[estimator.__name__]):\n",
    "                print(cur_params)\n",
    "                regressor = estimator(**cur_params)\n",
    "\n",
    "                # To collect MSE over each split\n",
    "                errors = []\n",
    "\n",
    "                # to collect accuracies\n",
    "                accuracies = []\n",
    "                top_1_accs = []\n",
    "\n",
    "                #validation_data = data_frame\n",
    "                #validation_data = validation_data.sort_values(by='points_won', ascending=False)\n",
    "\n",
    "                # Get train data\n",
    "                train_x = XTrain\n",
    "                train_y = yTrain\n",
    "                \n",
    "                # Validate over one season only\n",
    "                val_x = XVal\n",
    "                val_y = yVal\n",
    "                #val_y = val_y.reshape(val_y.shape[0], )\n",
    "                \n",
    "\n",
    "                if poly_fit is not None:\n",
    "                    train_x = poly_fit.fit_transform(train_x)\n",
    "                    val_x = poly_fit.fit_transform(val_x)\n",
    "\n",
    "                \n",
    "                shuffle_x, shuffle_y = shuffle(train_x, train_y)\n",
    "                \n",
    "\n",
    "                regressor.fit(shuffle_x, shuffle_y)\n",
    "                predicted_y = regressor.predict(val_x)\n",
    "\n",
    "                sorted_indices = np.argsort(predicted_y)[::-1]\n",
    "                correct_indices = np.arange(len(val_y))\n",
    "\n",
    "                curr_error = mean_squared_error(val_y, predicted_y)\n",
    "                errors.append(curr_error)\n",
    "                mean_error = np.average(errors)\n",
    "            \n",
    "                logging.info(f\"Params: {cur_params}, MSE over all splits is: {mean_error:.4f}\")\n",
    "                print(\n",
    "                    f\"Params: {cur_params}, MSE over all splits is: {mean_error:.4f}\")\n",
    "\n",
    "                if minimal_error is None or mean_error < minimal_error:\n",
    "                    minimal_error = mean_error\n",
    "                    best_estimator = estimator(**cur_params)\n",
    "        except Exception:\n",
    "            print(f\"Exception: {estimator}\")\n",
    "            continue\n",
    "            \n",
    "    return best_estimator\n",
    "\n",
    "def pipelinePCA(data_frame, estimators, params, filename, poly_fit):\n",
    "\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(filename=filename, filemode='w', level=logging.INFO)\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    minimal_error, best_estimator = None, None\n",
    "    \n",
    "    for estimator in estimators:\n",
    "        \n",
    "        try: \n",
    "            print(f\"Starting with estimator: {estimator.__name__}\")\n",
    "            logging.info(f\"Starting with estimator: {estimator.__name__}\")\n",
    "\n",
    "            for index, cur_params in enumerate(params[estimator.__name__]):\n",
    "                print(cur_params)\n",
    "                regressor = estimator(**cur_params)\n",
    "\n",
    "                # To collect MSE over each split\n",
    "                errors = []\n",
    "\n",
    "                # to collect accuracies\n",
    "                accuracies = []\n",
    "                top_1_accs = []\n",
    "\n",
    "                #validation_data = data_frame\n",
    "                #validation_data = validation_data.sort_values(by='points_won', ascending=False)\n",
    "\n",
    "                # Get train data\n",
    "                train_x = XTrain_pca\n",
    "                train_y = yTrain\n",
    "                \n",
    "                # Validate over one season only\n",
    "                val_x = XVal_pca\n",
    "                val_y = yVal\n",
    "                #val_y = val_y.reshape(val_y.shape[0], )\n",
    "                \n",
    "\n",
    "                if poly_fit is not None:\n",
    "                    train_x = poly_fit.fit_transform(train_x)\n",
    "                    val_x = poly_fit.fit_transform(val_x)\n",
    "\n",
    "                \n",
    "                shuffle_x, shuffle_y = shuffle(train_x, train_y)\n",
    "                \n",
    "\n",
    "                regressor.fit(shuffle_x, shuffle_y)\n",
    "                predicted_y = regressor.predict(val_x)\n",
    "\n",
    "                sorted_indices = np.argsort(predicted_y)[::-1]\n",
    "                correct_indices = np.arange(len(val_y))\n",
    "\n",
    "                curr_error = mean_squared_error(val_y, predicted_y)\n",
    "                errors.append(curr_error)\n",
    "                mean_error = np.average(errors)\n",
    "            \n",
    "                logging.info(f\"Params: {cur_params}, MSE over all splits is: {mean_error:.4f}\")\n",
    "                print(\n",
    "                    f\"Params: {cur_params}, MSE over all splits is: {mean_error:.4f}\")\n",
    "\n",
    "                if minimal_error is None or mean_error < minimal_error:\n",
    "                    minimal_error = mean_error\n",
    "                    best_estimator = estimator(**cur_params)\n",
    "        except Exception:\n",
    "            print(f\"Exception: {estimator}\")\n",
    "            continue\n",
    "            \n",
    "    return best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [Ridge]\n",
    "\n",
    "params = {Ridge.__name__: \n",
    "    [\n",
    "        {\n",
    "            'alpha': 1.0, 'solver':'svd'\n",
    "        },\n",
    "        {\n",
    "            'alpha': 50.0, 'solver':'svd'\n",
    "        },\n",
    "    ],\n",
    "         }\n",
    "\n",
    "estimators = [Ridge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with estimator: Ridge\n",
      "{'alpha': 1.0, 'solver': 'svd'}\n",
      "Params: {'alpha': 1.0, 'solver': 'svd'}, MSE over all splits is: 0.0011\n",
      "{'alpha': 50.0, 'solver': 'svd'}\n",
      "Params: {'alpha': 50.0, 'solver': 'svd'}, MSE over all splits is: 0.0013\n"
     ]
    }
   ],
   "source": [
    "best_estimator = pipeline(\n",
    "    data_frame= None,\n",
    "    estimators=estimators,\n",
    "    params=params,\n",
    "    filename=\"log_reg_poly_2.txt\",\n",
    "    # scaler=MinMaxScaler(),\n",
    "    poly_fit=PolynomialFeatures(degree=3, interaction_only=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with estimator: Ridge\n",
      "{'alpha': 1.0, 'solver': 'svd'}\n",
      "Params: {'alpha': 1.0, 'solver': 'svd'}, MSE over all splits is: 0.0017\n",
      "{'alpha': 50.0, 'solver': 'svd'}\n",
      "Params: {'alpha': 50.0, 'solver': 'svd'}, MSE over all splits is: 0.0015\n"
     ]
    }
   ],
   "source": [
    "best_estimator = pipelinePCA(\n",
    "    data_frame= None,\n",
    "    estimators=estimators,\n",
    "    params=params,\n",
    "    filename=\"log_reg_poly_2.txt\",\n",
    "    # scaler=MinMaxScaler(),\n",
    "    poly_fit=PolynomialFeatures(degree=3, interaction_only=True),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
