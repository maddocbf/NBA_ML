{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook aims to help set the hyperparameters to the ideal value to do this we use the training and tests sets to fit the hypterparameters, leaving a validation set to study once we have picked the best features (from the feature selection notebook) and hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import logging\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import PolynomialFeatures, normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor, AdaBoostRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain= pd.read_csv('../Data Scraping/DataTrainSet.csv')\n",
    "dataVal = pd.read_csv('../Data Scraping/DataValidateSet.csv')\n",
    "\n",
    "features = ['player','pos_x','age_x','team_id_x','g_x','mp_x','fg_per_poss','fga_per_poss','fg_pct_x'\n",
    "              ,'fg3_per_poss','fg3a_per_poss','fg3_pct_x','fg2_per_poss','fg2a_per_poss','fg2_pct_x','ft_per_poss'\n",
    "              ,'fta_per_poss','ft_pct_x','orb_per_poss','drb_per_poss','trb_per_poss','ast_per_poss','stl_per_poss'\n",
    "              ,'blk_per_poss','tov_per_poss','pf_per_poss','pts_per_poss','off_rtg','def_rtg','per','ts_pct'\n",
    "              ,'fg3a_per_fga_pct','fta_per_fga_pct','orb_pct','drb_pct','trb_pct','ast_pct','stl_pct','blk_pct'\n",
    "              ,'tov_pct','usg_pct','ows','dws','ws','ws_per_48','obpm','dbpm','bpm','vorp'\n",
    "              ,'mp_per_g','fg_per_g','fga_per_g','fg3_per_g','fg3a_per_g'\n",
    "              ,'fg2_per_g','fg2a_per_g','efg_pct','ft_per_g','fta_per_g','orb_per_g','drb_per_g','trb_per_g'\n",
    "              ,'ast_per_g','stl_per_g','blk_per_g','tov_per_g','pf_per_g','pts_per_g','fg_per_mp','fga_per_mp'\n",
    "              ,'fg3_per_mp','fg3a_per_mp','fg2_per_mp','fg2a_per_mp','ft_per_mp','fta_per_mp','orb_per_mp'\n",
    "              ,'drb_per_mp','trb_per_mp','ast_per_mp','stl_per_mp','blk_per_mp','tov_per_mp','pf_per_mp','pts_per_mp'\n",
    "              ,'votes_first','points_won']\n",
    "\n",
    "feat = features[4:-2]\n",
    "XTrainDF = dataTrain[feat]\n",
    "XValDF = dataVal[feat]\n",
    "XTrainDF = XTrainDF.fillna(0)\n",
    "XValDF = XValDF.fillna(0)\n",
    "\n",
    "XTrain = XTrainDF.to_numpy()\n",
    "XVal = XValDF.to_numpy()\n",
    "\n",
    "yTrain = dataTrain['points_won'].to_numpy()\n",
    "yVal = dataVal['points_won'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data_frame, estimators, params, filename, poly_fit):\n",
    "\n",
    "    print(\"here: 1\")\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(filename=filename, filemode='w', level=logging.INFO)\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    minimal_error, best_estimator = None, None\n",
    "    \n",
    "    for estimator in estimators:\n",
    "        \n",
    "        try: \n",
    "            print(f\"Starting with estimator: {estimator.__name__}\")\n",
    "            logging.info(f\"Starting with estimator: {estimator.__name__}\")\n",
    "\n",
    "            for index, cur_params in enumerate(params[estimator.__name__]):\n",
    "                print(cur_params)\n",
    "                regressor = estimator(**cur_params)\n",
    "                print(\"here: 2\")\n",
    "\n",
    "                # To collect MSE over each split\n",
    "                errors = []\n",
    "\n",
    "                # to collect accuracies\n",
    "                accuracies = []\n",
    "                top_1_accs = []\n",
    "\n",
    "                #validation_data = data_frame\n",
    "                #validation_data = validation_data.sort_values(by='points_won', ascending=False)\n",
    "\n",
    "                # Get train data\n",
    "                train_x = XTrain\n",
    "                train_y = yTrain\n",
    "                print(\"here: 3\")\n",
    "                \n",
    "                # Validate over one season only\n",
    "                val_x = XVal\n",
    "                val_y = yVal\n",
    "                #val_y = val_y.reshape(val_y.shape[0], )\n",
    "                print(\"here: 4\")\n",
    "                \n",
    "\n",
    "                if poly_fit is not None:\n",
    "                    train_x = poly_fit.fit_transform(train_x)\n",
    "                    val_x = poly_fit.fit_transform(val_x)\n",
    "\n",
    "                \n",
    "                shuffle_x, shuffle_y = shuffle(train_x, train_y)\n",
    "                \n",
    "\n",
    "                regressor.fit(shuffle_x, shuffle_y)\n",
    "                predicted_y = regressor.predict(val_x)\n",
    "\n",
    "                sorted_indices = np.argsort(predicted_y)[::-1]\n",
    "                correct_indices = np.arange(len(val_y))\n",
    "\n",
    "                curr_error = mean_squared_error(val_y, predicted_y)\n",
    "                errors.append(curr_error)\n",
    "                mean_error = np.average(errors)\n",
    "            \n",
    "                logging.info(f\"Params: {cur_params}, MSE over all splits is: {mean_error:.4f}\")\n",
    "                print(\n",
    "                    f\"Params: {cur_params}, MSE over all splits is: {mean_error:.4f}\")\n",
    "\n",
    "                if minimal_error is None or mean_error < minimal_error:\n",
    "                    minimal_error = mean_error\n",
    "                    best_estimator = estimator(**cur_params)\n",
    "        except Exception:\n",
    "            print(f\"Exception: {estimator}\")\n",
    "            continue\n",
    "            \n",
    "    return best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_x = poly_fit.fit_transform(XTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [Ridge]\n",
    "\n",
    "params = {Ridge.__name__: \n",
    "    [\n",
    "        {\n",
    "            'alpha': 1.0, 'solver':'svd'\n",
    "        },\n",
    "        {\n",
    "            'alpha': 10.0, 'solver':'svd'\n",
    "        },\n",
    "        {\n",
    "            'alpha': 30.0, 'solver':'svd'\n",
    "        },\n",
    "        {\n",
    "            'alpha': 50.0, 'solver':'svd'\n",
    "        },\n",
    "    ],\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here: 1\n",
      "Starting with estimator: Ridge\n",
      "{'alpha': 1.0, 'solver': 'svd'}\n",
      "here: 2\n",
      "here: 3\n",
      "here: 4\n",
      "Params: {'alpha': 1.0, 'solver': 'svd'}, MSE over all splits is: 0.0023\n",
      "{'alpha': 10.0, 'solver': 'svd'}\n",
      "here: 2\n",
      "here: 3\n",
      "here: 4\n",
      "Params: {'alpha': 10.0, 'solver': 'svd'}, MSE over all splits is: 0.0018\n",
      "{'alpha': 30.0, 'solver': 'svd'}\n",
      "here: 2\n",
      "here: 3\n",
      "here: 4\n",
      "Params: {'alpha': 30.0, 'solver': 'svd'}, MSE over all splits is: 0.0018\n",
      "{'alpha': 50.0, 'solver': 'svd'}\n",
      "here: 2\n",
      "here: 3\n",
      "here: 4\n",
      "Params: {'alpha': 50.0, 'solver': 'svd'}, MSE over all splits is: 0.0017\n"
     ]
    }
   ],
   "source": [
    "best_estimator = pipeline(\n",
    "    data_frame=val_data,\n",
    "    estimators=estimators,\n",
    "    params=params,\n",
    "    filename=\"log_reg_poly_2.txt\",\n",
    "    # scaler=MinMaxScaler(),\n",
    "    poly_fit=PolynomialFeatures(degree=2, interaction_only=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [RandomForestRegressor]\n",
    "\n",
    "params = {RandomForestRegressor.__name__: \n",
    "    [ \n",
    "        {\n",
    "            'n_estimators':100, \n",
    "            'min_samples_split':2,\n",
    "            'min_weight_fraction_leaf':0\n",
    "        },\n",
    "        {\n",
    "            'n_estimators':100, \n",
    "            'min_samples_split':2,\n",
    "            'warm_start':'True'\n",
    "        },\n",
    "        {\n",
    "            'n_estimators':100, \n",
    "            'min_samples_split':2,\n",
    "            'min_impurity_decrease':0.0\n",
    "        },\n",
    "    \n",
    "    ],\n",
    "         }\n",
    "\n",
    "best_estimator = pipeline(\n",
    "    data_frame=train_data,\n",
    "    estimators=estimators,\n",
    "    params=params,\n",
    "    filename=\"log_reg_poly_2.txt\",\n",
    "    # scaler=MinMaxScaler(),\n",
    "    poly_fit=PolynomialFeatures(degree=2, interaction_only=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with estimator: SGDRegressor\n",
      "{'loss': 'squared_epsilon_insensitive', 'epsilon': 0.0001, 'learning_rate': 'adaptive', 'eta0': 0.01}\n",
      "Params: {'loss': 'squared_epsilon_insensitive', 'epsilon': 0.0001, 'learning_rate': 'adaptive', 'eta0': 0.01}, MSE over all splits is: 0.0175\n",
      "{'loss': 'squared_epsilon_insensitive', 'epsilon': 0.0001, 'learning_rate': 'adaptive', 'eta0': 0.01, 'warm_start': 'True'}\n",
      "Params: {'loss': 'squared_epsilon_insensitive', 'epsilon': 0.0001, 'learning_rate': 'adaptive', 'eta0': 0.01, 'warm_start': 'True'}, MSE over all splits is: 0.0178\n",
      "{'loss': 'squared_epsilon_insensitive', 'epsilon': 0.0001, 'learning_rate': 'adaptive', 'eta0': 0.01}\n",
      "Params: {'loss': 'squared_epsilon_insensitive', 'epsilon': 0.0001, 'learning_rate': 'adaptive', 'eta0': 0.01}, MSE over all splits is: 0.0174\n"
     ]
    }
   ],
   "source": [
    "estimators = [SGDRegressor]\n",
    "\n",
    "params = {SGDRegressor.__name__: \n",
    "    [ \n",
    "        {\n",
    "            'loss':'squared_epsilon_insensitive',\n",
    "            'epsilon':0.0001,\n",
    "            'learning_rate':'adaptive',\n",
    "            'eta0':0.01\n",
    "            \n",
    "        },\n",
    "        {\n",
    "            'loss':'squared_epsilon_insensitive',\n",
    "            'epsilon':0.0001,\n",
    "            'learning_rate':'adaptive',\n",
    "            'eta0':0.01,\n",
    "            'warm_start': 'True',\n",
    "        },\n",
    "        {\n",
    "            'loss':'squared_epsilon_insensitive',\n",
    "            'epsilon':0.0001,\n",
    "            'learning_rate':'adaptive',\n",
    "            'eta0':0.01\n",
    "        },\n",
    "    \n",
    "    ],\n",
    "         }\n",
    "\n",
    "best_estimator = pipeline(\n",
    "    data_frame=train_data,\n",
    "    estimators=estimators,\n",
    "    params=params,\n",
    "    filename=\"log_reg_poly_2.txt\",\n",
    "    # scaler=MinMaxScaler(),\n",
    "    poly_fit=PolynomialFeatures(degree=2, interaction_only=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with estimator: GradientBoostingRegressor\n",
      "{'loss': 'ls', 'learning_rate': 0.5, 'n_estimators': 100}\n",
      "Params: {'loss': 'ls', 'learning_rate': 0.5, 'n_estimators': 100}, MSE over all splits is: 0.0001\n",
      "{'loss': 'ls', 'learning_rate': 0.5, 'n_estimators': 300}\n",
      "Params: {'loss': 'ls', 'learning_rate': 0.5, 'n_estimators': 300}, MSE over all splits is: 0.0000\n",
      "{'loss': 'ls', 'learning_rate': 0.1, 'n_estimators': 100}\n",
      "Params: {'loss': 'ls', 'learning_rate': 0.1, 'n_estimators': 100}, MSE over all splits is: 0.0026\n",
      "{'loss': 'ls', 'learning_rate': 0.1, 'n_estimators': 300}\n",
      "Params: {'loss': 'ls', 'learning_rate': 0.1, 'n_estimators': 300}, MSE over all splits is: 0.0003\n"
     ]
    }
   ],
   "source": [
    "estimators = [GradientBoostingRegressor]\n",
    "\n",
    "params = {GradientBoostingRegressor.__name__: \n",
    "    [ \n",
    "        {\n",
    "            'loss':'ls',\n",
    "            'learning_rate':0.5,\n",
    "            'n_estimators':100\n",
    "        },\n",
    "        {\n",
    "            'loss':'ls',\n",
    "            'learning_rate':0.5,\n",
    "            'n_estimators':300\n",
    "        },\n",
    "        {\n",
    "            'loss':'ls',\n",
    "            'learning_rate':0.1,\n",
    "            'n_estimators':100\n",
    "        },\n",
    "        {\n",
    "            'loss':'ls',\n",
    "            'learning_rate':0.1,\n",
    "            'n_estimators':300\n",
    "        },\n",
    "    \n",
    "    ],\n",
    "         }\n",
    "\n",
    "best_estimator = pipeline(\n",
    "    data_frame=train_data,\n",
    "    estimators=estimators,\n",
    "    params=params,\n",
    "    filename=\"log_reg_poly_2.txt\",\n",
    "    # scaler=MinMaxScaler(),\n",
    "    poly_fit=PolynomialFeatures(degree=2, interaction_only=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with estimator: MLPRegressor\n",
      "{'hidden_layer_sizes': (1000,), 'activation': 'relu', 'solver': 'lbfgs'}\n",
      "Params: {'hidden_layer_sizes': (1000,), 'activation': 'relu', 'solver': 'lbfgs'}, MSE over all splits is: 0.0094\n",
      "{'hidden_layer_sizes': (1000,), 'activation': 'relu', 'solver': 'lbfgs'}\n",
      "Params: {'hidden_layer_sizes': (1000,), 'activation': 'relu', 'solver': 'lbfgs'}, MSE over all splits is: 0.0097\n",
      "{'hidden_layer_sizes': (1000,), 'activation': 'relu', 'solver': 'lbfgs'}\n",
      "Params: {'hidden_layer_sizes': (1000,), 'activation': 'relu', 'solver': 'lbfgs'}, MSE over all splits is: 0.0098\n"
     ]
    }
   ],
   "source": [
    "estimators = [MLPRegressor]\n",
    "\n",
    "params = {MLPRegressor.__name__: \n",
    "    [ \n",
    "        {\n",
    "            'hidden_layer_sizes':(1000,),\n",
    "            'activation':'relu',\n",
    "            'solver':'lbfgs'\n",
    "        },\n",
    "        {\n",
    "            'hidden_layer_sizes':(1000,),\n",
    "            'activation':'relu',\n",
    "            'solver':'lbfgs'\n",
    "        },\n",
    "        {\n",
    "            'hidden_layer_sizes':(1000,),\n",
    "            'activation':'relu',\n",
    "            'solver':'lbfgs'\n",
    "        },\n",
    "    \n",
    "    ],\n",
    "         }\n",
    "\n",
    "best_estimator = pipeline(\n",
    "    data_frame=train_data,\n",
    "    estimators=estimators,\n",
    "    params=params,\n",
    "    filename=\"log_reg_poly_2.txt\",\n",
    "    # scaler=MinMaxScaler(),\n",
    "    poly_fit=PolynomialFeatures(degree=2, interaction_only=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
